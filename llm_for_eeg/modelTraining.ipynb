{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-01 00:29:56.239504: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 00:29:56.772099: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/cuda/lib:/usr/local/cuda/lib64:\n",
      "2023-06-01 00:29:56.772157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/cuda/lib:/usr/local/cuda/lib64:\n",
      "2023-06-01 00:29:56.772161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('sam_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_data, f)\n",
    "with open('../sam_dataset.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc(data_array):\n",
    "    truncated_sentence = []\n",
    "    for i in range(len(data_array)):\n",
    "        for j in range(len(data_array[i])-1):\n",
    "            tmp = data_array[i][j] + data_array[i][j+1]\n",
    "            # print(len(train_data[i][j]),len(train_data[i][j+1]))\n",
    "            # print(len(tmp))\n",
    "            truncated_sentence.append(tmp)\n",
    "    return truncated_sentence\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2052\n",
      "108234\n",
      "====================\n",
      "256\n",
      "13236\n",
      "====================\n",
      "257\n",
      "13320\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    " # Path to the output file\n",
    "train_data1 = train_data[0:int(len(train_data)*0.8)]\n",
    "tmp = train_data[int(len(train_data)*0.8):]\n",
    "test_data1 = tmp[0:int(len(tmp)*0.5)]\n",
    "\n",
    "val_data1 = tmp[int(len(tmp)*0.5):]\n",
    "trunc_train = trunc(train_data1)\n",
    "file_path = \"train2.txt\" \n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in trunc_train:\n",
    "        # Convert each element in the row to a string and join them with a delimiter\n",
    "        line = \"\".join(str(elem) for elem in row)\n",
    "        file.write(line + \"\\n\")\n",
    "print(len(train_data1))\n",
    "print(len(trunc_train))\n",
    "print(\"=\"*20)\n",
    "file_path = \"test2.txt\" \n",
    "trunc_test = trunc(test_data1)\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in trunc_test:\n",
    "        # Convert each element in the row to a string and join them with a delimiter\n",
    "        line = \"\".join(str(elem) for elem in row)\n",
    "        file.write(line + \"\\n\")\n",
    "print(len(test_data1))\n",
    "print(len(trunc_test))\n",
    "print(\"=\"*20)\n",
    "file_path = \"val2.txt\" \n",
    "trunc_val = trunc(val_data1)\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    for row in trunc_val:\n",
    "        # Convert each element in the row to a string and join them with a delimiter\n",
    "        line = \"\".join(str(elem) for elem in row)\n",
    "        file.write(line + \"\\n\")\n",
    "print(len(val_data1))\n",
    "print(len(trunc_val))\n",
    "print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/john_zhang/.cache/huggingface/datasets/text/default-f85a1b490e1abc0f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    }
   ],
   "source": [
    "files = [\"data/train2.txt\", \"data/test2.txt\", \"data/val2.txt\"] # train3.txt, etc.\n",
    "dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['train']['text'][0])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"output.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training tokenizer\n",
    "# tokenizer = BertWordPieceTokenizer()\n",
    "# # train the tokenizer\n",
    "# tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# # enable truncation up to the maximum 512 tokens\n",
    "# tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training tokenizer\n",
    "\n",
    "# model_path = \"pretrained-bert\"\n",
    "# if not os.path.isdir(model_path):\n",
    "#   os.mkdir(model_path)\n",
    "# # save the tokenizer  \n",
    "# tokenizer.save_model(model_path)\n",
    "# # dumping some of the tokenizer config to config file, \n",
    "# # including special tokens, whether to lower case and the maximum sequence length\n",
    "# with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "#   tokenizer_cfg = {\n",
    "#       \"do_lower_case\": True,\n",
    "#       \"unk_token\": \"[UNK]\",\n",
    "#       \"sep_token\": \"[SEP]\",\n",
    "#       \"pad_token\": \"[PAD]\",\n",
    "#       \"cls_token\": \"[CLS]\",\n",
    "#       \"mask_token\": \"[MASK]\",\n",
    "#       \"model_max_length\": max_length,\n",
    "#       \"max_len\": max_length,\n",
    "#   }\n",
    "#   json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"pretrained-bert\"\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bpe tokenizer\n",
    "# # from transformers import RobertaTokenizerFast\n",
    "# # tokenizer_folder = \"./bpe\"\n",
    "\n",
    "# # tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, return_special_tokens_mask=True, max_length=512)  \n",
    "# # bpe tokenizer\n",
    "# from transformers import RobertaTokenizerFast\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# tokenizer = Tokenizer(BPE())\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "# tokenizer.train(files=[\"output.txt\"], trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tokenizer.save(\"bpev2.tokenizer.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file('bpev2.tokenizer.json')\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "tokenizer = fast_tokenizer\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.mask_token = \"[MASK]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  266,   401,  3825,  7124,  3391,   115,  1410,  2722,   241,   480,\n",
       "          285,  2958,  3255,   777,   187,  1989,   818,   627,  7185, 13462,\n",
       "         1791,   327,  4948,  1604,   201,   997,  1049,  1206,   224,   290,\n",
       "           73,   558,   883,    59,   287,  3510,   239,   491, 11192, 12528,\n",
       "          730,   213, 10145,  6656,   440, 10648,  4657,  3607,   799,  4288,\n",
       "        14950,    42, 16986,  2563,   295,   632,   390,   199,  7639,  4259,\n",
       "         4971,  2148,   373,  7250,   146,  1071,   217,  1129,  4365,  1145,\n",
       "          389,    46,  5131,    51,  1558,   309, 13945,   657,  2821,   299,\n",
       "         1798,   229,   408,   252,  1575,  8697,  2059,    18,   327,  2516,\n",
       "          389, 17804,  9430,   135, 18923,  1311,   219,   308,   485, 10935,\n",
       "           85,  1272,  2641,    52,   218,   187,   330,   287,  2010,   126,\n",
       "         6372,   217,   206,  4769,   679,  1164,    28, 19408,  6676,   312,\n",
       "         2604, 10687,  2045,   913,   113,  1916,   225,   402,   423,   403,\n",
       "          525,  2194,   451,   330,   237,  3018,     7,  3710,     6,   573,\n",
       "         3279,  3592,   902,   256, 26373,  1371, 28403,   587,   117,   900,\n",
       "          257,   100,  1812,  7497,  3919,   233,   325,  1235, 11048,  2116,\n",
       "           15,   140,  7051,   779, 15928,  1377,  2893,    69,  3225,   518,\n",
       "         1027,  1551,   235,  4694,   235,   616,  2122,    50,  2179,  6257,\n",
       "          482,   349, 12773,    93, 13444,   334,  5607,  2185,    15,  3984,\n",
       "          389,   159,   369,    13,   340, 12813,  1907,  5041,    56,  1166,\n",
       "          662,   641,   153,  1026,   726,  3707,   309,    24,  1849,  2474,\n",
       "         1113,  1495,  2097, 24702,   175,  2097,   229,   163,  1291,   591,\n",
       "         5137,  2097,   961,  1888,    80,   349,    28,  1516, 15380,   957,\n",
       "        23572,  2674,   197,   755, 20462, 11328, 15347,    68,  3894,  1026,\n",
       "           68,  3430,   105,   802,   490,   200,  1408,   249,    78,  7645,\n",
       "         1254,   256,  2651,  7689,    39, 18300, 25295,   862, 11132,  2208,\n",
       "         1410,   145,   412, 15689,  3198, 27147,   318, 26212,   120,  1101,\n",
       "         2502,   410,   221,  3183,  1466,   141,  2352,   241, 15261,    56,\n",
       "         3172, 13377,   195,    46,  1795,  5803,   316,  9320, 14578,  9560,\n",
       "          310,   884,   619,  8535,  3278, 15058,   577,    53,  4738,  2104,\n",
       "          479, 26387,   129,   584,  1018,   285,  1031,  2523,    18,  2415,\n",
       "          483,    94,  4402,   676,   115,   581,   676,   539,  7472,  1275,\n",
       "          350,  4051,   520,   484,   308,   479,   123,  2093,  1593,    68,\n",
       "         1380,   419,   125,   218,   305,  3875,  1347,   674,  9336,   135,\n",
       "         2607,   561, 23391,  1752,  2083,    81,  3678,   327,   190,  1570,\n",
       "        14377,  2084, 15385,   762,  8537,  2939,  5579, 18914, 24517,  2313,\n",
       "         1286,   568,   216, 11596, 14005, 23217,   155,  4021,    54,  1493,\n",
       "           80, 12352,   305,   412,  3032,  7151,   146,   372,  4437,   213,\n",
       "         1113,  9003,  1166,   923,   537,   123,   976,   258,   702, 10645,\n",
       "          175,  1668,  7346,   551,   551,  7213,   363,  3084,  2628,    97,\n",
       "           33,  6233,  1842,   504,  6970,   467,  1997,   117,   555,    53,\n",
       "          187,   444, 11681,   289,  1224,    77, 20748,   117, 10641,  1740,\n",
       "          165,  4192,   102,  4202,   272,   463,   436,  1316,  1127, 20008,\n",
       "          230,   181,   921,  7377,  2295,  3454,   168,  2383, 10374,   840,\n",
       "          130,    58,    41,  1173,   186,   585,  3111,   958, 16426,  2036,\n",
       "          567,  8214,  1076,    59,  1625,  9468,   640,    66,  2148,   287,\n",
       "         5194,  3248,    88, 26586,   611,  6324,    41,  3699,   895,   239,\n",
       "         1280,  4110,   377,  1032, 19618,  2827,   341, 13845,  2272,    54,\n",
       "         1891,   503, 14443,   144,  2975,   165,   273,    82,  5595, 26412,\n",
       "          102,  1223,  2702,   133,  3485,  7144,   821,   712,    25,  2270,\n",
       "        13960,    24, 27038,   736,  1944,   183, 20421,  2297, 23441,   292,\n",
       "          181,    98])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121311, 13479)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT\n",
    "\n",
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, \n",
    "                          max_position_embeddings=max_length,\n",
    "                          num_attention_heads=1,\n",
    "                          num_encoder_layers=3,\n",
    "                          type_vocab_size=1,\n",
    "                          hidden_size = 250,\n",
    "                          intermediate_size = 307)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=customBert\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjagohion\u001b[0m (\u001b[33mai_essay_gem\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "%env WANDB_PROJECT=customBert\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model_path = \"pretrained-bert\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=800,            # number of training epochs\n",
    "    per_device_train_batch_size=2, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=100,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert_v1\",\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=  3         # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 121311\n",
      "  Num Epochs = 800\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 6065600\n",
      "  Number of trainable parameters = 109514298\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/john_zhang/EEG_NLP/llm_for_eeg/wandb/run-20230601_003154-nws96tb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai_essay_gem/customBert/runs/nws96tb5' target=\"_blank\">bert_v1</a></strong> to <a href='https://wandb.ai/ai_essay_gem/customBert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai_essay_gem/customBert' target=\"_blank\">https://wandb.ai/ai_essay_gem/customBert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai_essay_gem/customBert/runs/nws96tb5' target=\"_blank\">https://wandb.ai/ai_essay_gem/customBert/runs/nws96tb5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='6065600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     38/6065600 00:17 < 838:47:23, 2.01 it/s, Epoch 0.00/800]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13479\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13479\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▅▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▂▄▅▅▅▆▆▆▆▆▆▆▇▇███████████▇█▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>eval/samples_per_second</td><td>▇█▇▅▄▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>eval/steps_per_second</td><td>▇█▇▅▄▄▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>8.44235</td></tr><tr><td>eval/runtime</td><td>2.2862</td></tr><tr><td>eval/samples_per_second</td><td>112.413</td></tr><tr><td>eval/steps_per_second</td><td>2.187</td></tr><tr><td>train/epoch</td><td>4.99</td></tr><tr><td>train/global_step</td><td>720</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>8.5183</td></tr><tr><td>train/total_flos</td><td>3032119443456000.0</td></tr><tr><td>train/train_loss</td><td>8.78629</td></tr><tr><td>train/train_runtime</td><td>538.9806</td></tr><tr><td>train/train_samples_per_second</td><td>21.411</td></tr><tr><td>train/train_steps_per_second</td><td>1.336</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_customT_v2</strong> at: <a href='https://wandb.ai/ai_essay_gem/customBert/runs/4kj879lg' target=\"_blank\">https://wandb.ai/ai_essay_gem/customBert/runs/4kj879lg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230515_142125-4kj879lg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri May 12 17:01:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.46       Driver Version: 495.46       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 48%   65C    P2   117W / 350W |   5119MiB / 24260MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:21:00.0 Off |                  N/A |\n",
      "|  0%   60C    P2   117W / 350W |  15796MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1866      G   /usr/lib/xorg/Xorg                 56MiB |\n",
      "|    0   N/A  N/A      2264      G   /usr/bin/gnome-shell               10MiB |\n",
      "|    0   N/A  N/A    359015      C   ...user/miniconda/bin/python     1705MiB |\n",
      "|    0   N/A  N/A   2629036      C   ...a/envs/eeg_nlp/bin/python     1307MiB |\n",
      "|    0   N/A  N/A   3170783      C   ...user/miniconda/bin/python      627MiB |\n",
      "|    0   N/A  N/A   3549287      C   /usr/bin/python3                 1409MiB |\n",
      "|    1   N/A  N/A      1866      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2742478      C   ...ocal/anaconda3/bin/python    15057MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eded5328b539aa62c7d54b8449ea7e31e09bb634b1baaf115ac6e33bac29d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
