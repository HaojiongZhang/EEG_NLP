{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data prep for dimensionality reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import mne\n",
    "from pyts.approximation import SymbolicAggregateApproximation # https://pyts.readthedocs.io/en/stable/auto_examples/approximation/plot_sax.html\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "N_BINS = 8\n",
    "SAMPLING_RATE = 128\n",
    "\n",
    "DATA_CHANNEL = 'EEG FZ-REF'\n",
    "REFERENCE_CHANNEL = 'EEG CZ-REF'\n",
    "\n",
    "# TRANSFORMER INPUT CONFIG\n",
    "ALPHABET_LEN = .03 #sec : 30 milliseconds\n",
    "WORD_LEN = 10 #sec : 10 seconds\n",
    "WORDS_PER_SENT = 10\n",
    "\n",
    "abnorm_datapath = r\"/mnt/ssd_4tb_0/shared/TUH_EEG_ABNORMAL/v2.0.0/edf/eval/abnormal\"\n",
    "norm_datapath = r\"/mnt/ssd_4tb_0/shared/TUH_EEG_ABNORMAL/v2.0.0/edf/eval/normal\"\n",
    "\n",
    "abnorm_edf_files = glob.glob(f'{abnorm_datapath}/**/*.edf', recursive=True)\n",
    "norm_edf_files = glob.glob(f'{norm_datapath}/**/*.edf', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = r\"/mnt/ssd_4tb_0/shared/TUH_EEG_ABNORMAL/\"\n",
    "edf_files = glob.glob(f'{DATA_PATH}/**/*.edf', recursive=True)\n",
    "TEST_FILES = edf_files[::7]\n",
    "TRAIN_FILES = list(filter(lambda x: x not in TEST_FILES, edf_files))\n",
    "TO_TRANSFORM_FILES = TRAIN_FILES[::10]\n",
    "\n",
    "ALL_FILES = edf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LABELS = []\n",
    "for files in TEST_FILES:\n",
    "    t = files.split('/')\n",
    "    label = t[8]\n",
    "    TEST_LABELS.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TEST_FILES), len(TEST_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "folder_path = '/home/john_zhang/EEG_NLP/llm_for_eeg/eval'  # Replace with the path to your folder\n",
    "file_extension = '.pkl'\n",
    "\n",
    "# Get a list of all .pkl files in the folder\n",
    "file_list = [file for file in os.listdir(folder_path) if file.endswith(file_extension)]\n",
    "\n",
    "# Create an empty list to store the data from each file\n",
    "data_array = []\n",
    "\n",
    "# Iterate over the files and load the data into the array\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        data_array.append(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_array),len(TEST_FILES),len(TEST_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample n from normal/abnormal\n",
    "sample_size = 50\n",
    "\n",
    "import numpy as np\n",
    "# arr1 = np.array(patient_id)\n",
    "arr2 = np.array(TEST_LABELS)\n",
    "# Find indices of sequences with label A and B\n",
    "indices_A = np.where(arr2 == 'abnormal')[0]\n",
    "indices_B = np.where(arr2 == 'normal')[0]\n",
    "\n",
    "\n",
    "# Randomly shuffle the indices\n",
    "np.random.shuffle(indices_A)\n",
    "np.random.shuffle(indices_B)\n",
    "\n",
    "# print(indices_A[:100])\n",
    "# print(indices_B)\n",
    "# print(np.intersect1d(indices_A,indices_B))\n",
    "\n",
    "abnorm_samples = indices_A[:sample_size]\n",
    "norm_samples = indices_B[:sample_size]\n",
    "\n",
    "sample_labels = ['abnormal']*len(abnorm_samples) + ['normal']*len(norm_samples)\n",
    "\n",
    "\n",
    "# Select the first 100 sequences with label A and B\n",
    "# abnorm = [train_data[x] for x in indices_A[:nums]]\n",
    "# abnorm_patient_id = arr1[indices_A[:nums]]\n",
    "\n",
    "# norm = [train_data[x] for x in indices_B[:nums]]\n",
    "# norm_patient_id = arr1[indices_B[:nums]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = 1000\n",
    "TOKEN_LENGTH = 30 #ms\n",
    "WORD_LENGTH = 10*s #s\n",
    "SENTENCE_LENGTH = 100*s #s\n",
    "\n",
    "TOKENIZER_BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "labels = []\n",
    "patient_id = []\n",
    "for i in abnorm_samples:\n",
    "    cur_sbj = data_array[i]\n",
    "    wrd_step = int(WORD_LENGTH/TOKEN_LENGTH)\n",
    "    words = [\"\".join(cur_sbj[i:i+wrd_step]) for i in range(0,len(cur_sbj),wrd_step)]\n",
    "    \n",
    "    snt_step = int(SENTENCE_LENGTH/WORD_LENGTH)\n",
    "    #print(words[0])\n",
    "    sentences = [\" \".join(words[i:i+snt_step]) for i in range(0,len(words),snt_step)]\n",
    "    labels += len(sentences)*[TEST_LABELS[i]]\n",
    "    \n",
    "    patient_id.append(len(sentences))\n",
    "   \n",
    "    \n",
    "    train_data.append(sentences)\n",
    "for i in norm_samples:\n",
    "    cur_sbj = data_array[i]\n",
    "    wrd_step = int(WORD_LENGTH/TOKEN_LENGTH)\n",
    "    words = [\"\".join(cur_sbj[i:i+wrd_step]) for i in range(0,len(cur_sbj),wrd_step)]\n",
    "    \n",
    "    snt_step = int(SENTENCE_LENGTH/WORD_LENGTH)\n",
    "    #print(words[0])\n",
    "    \n",
    "    sentences = [\" \".join(words[i:i+snt_step]) for i in range(0,len(words),snt_step)]\n",
    "    labels += len(sentences)*[TEST_LABELS[i]]\n",
    "    patient_id.append(len(sentences))\n",
    "   \n",
    "    \n",
    "    train_data.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.count('normal'),labels.count('abnormal'),len(patient_id), len(train_data), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_2d_array_to_txt(array, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for row in array:\n",
    "            file.write(''.join(row) + '\\n')\n",
    "\n",
    "tmp_arr = []\n",
    "for sent in train_data:\n",
    "    tmp_arr += sent\n",
    "\n",
    "# Specify the filename\n",
    "output_file = 'train_data3.txt'\n",
    "\n",
    "# Write the array to the .txt file\n",
    "write_2d_array_to_txt(tmp_arr, output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running test data through the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-20 13:19:21.681108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-20 13:19:22.939667: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/cuda/lib:/usr/local/cuda/lib64:\n",
      "2023-06-20 13:19:22.940195: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/cuda/lib:/usr/local/cuda/lib64:\n",
      "2023-06-20 13:19:22.940202: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/home/john_zhang/.conda/envs/eeg_nlp/lib/python3.9/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/john_zhang/EEG_NLP/llm_for_eeg/pretrained-bert/checkpoint-7000/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /home/john_zhang/EEG_NLP/llm_for_eeg/pretrained-bert/checkpoint-7000/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /home/john_zhang/EEG_NLP/llm_for_eeg/pretrained-bert/checkpoint-7000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file /home/john_zhang/EEG_NLP/llm_for_eeg/pretrained-bert/checkpoint-7000/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model_path = \"/home/john_zhang/EEG_NLP/llm_for_eeg/pretrained-bert\"\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-7000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file('bpev2.tokenizer.json')\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "tokenizer = fast_tokenizer\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.mask_token = \"[MASK]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"output.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/john_zhang/.cache/huggingface/datasets/text/default-2b0679017126b8ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1942.71it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/john_zhang/.cache/huggingface/datasets/text/default-2b0679017126b8ad/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 805.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files=[\"train_data3.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# tokenizing the train dataset\n",
    "test_dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 5168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5168"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 157, 5845,  157,  ...,    3,    3,    3],\n",
      "        [   6, 1364,  303,  ...,  765,  583,    3],\n",
      "        [   8,  351,   85,  ...,    3,    3,    3],\n",
      "        ...,\n",
      "        [  38,  296,  482,  ...,    3,    3,    3],\n",
      "        [  14, 7257, 1558,  ...,    3,    3,    3],\n",
      "        [5591,   57, 2164,  ...,    3,    3,    3]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = test_dataset['train']['input_ids'][0:47]  # Add a dimension for batch_size\n",
    "print(input_ids)\n",
    "attention_mask = test_dataset['train']['attention_mask'][0:47]  # Add a dimension for batch_size\n",
    "print(attention_mask)\n",
    "output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**smaller samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:41<00:00,  9.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "start = 0\n",
    "avg_output = []\n",
    "labels_small = []\n",
    "\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    end = start + patient_id[i]\n",
    "    #print(start,end)\n",
    "    input_ids = test_dataset['train']['input_ids'][start:end]  \n",
    "    attention_mask = test_dataset['train']['attention_mask'][start:end]\n",
    "    start += patient_id[i]\n",
    "    output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True) \n",
    "    #print(output.hidden_states[-1].shape)\n",
    "    tmp_avg1 = np.mean(output.hidden_states[-1].detach().numpy(),axis=1)\n",
    "    #print(tmp_avg1.shape)\n",
    "    tmp_avg2 = np.mean(tmp_avg1,axis = 0)\n",
    "    #print(tmp_avg2.shape)\n",
    "    avg_output.append(tmp_avg2)\n",
    "    labels_small.append(sample_labels[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_output),len(labels_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:04<00:00,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # averaging individual segments, then average into (batch_size, hidden_output_size)\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# start = 0\n",
    "# avg_output = []\n",
    "# labels_small = []\n",
    "# for i in tqdm(range(len(abnorm))):\n",
    "#     end = start + abnorm_patient_id[i]\n",
    "#     #print(start,end)\n",
    "#     input_ids = test_dataset['train']['input_ids'][start:end]  \n",
    "#     attention_mask = test_dataset['train']['attention_mask'][start:end]\n",
    "#     start += abnorm_patient_id[i]\n",
    "#     output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True) \n",
    "#     #print(output.hidden_states[-1].shape)\n",
    "#     tmp_avg1 = np.mean(output.hidden_states[-1].detach().numpy(),axis=1)\n",
    "#     #print(tmp_avg1.shape)\n",
    "#     tmp_avg2 = np.mean(tmp_avg1,axis = 0)\n",
    "#     #print(tmp_avg2.shape)\n",
    "#     avg_output.append(tmp_avg2)\n",
    "#     labels_small.append('abnormal')\n",
    "\n",
    "# # for i in tqdm(range(len(norm))):\n",
    "# #     end = start + norm_patient_id[i]\n",
    "# #     #print(start,end)\n",
    "# #     input_ids = test_dataset['train']['input_ids'][start:end]  \n",
    "# #     attention_mask = test_dataset['train']['attention_mask'][start:end]\n",
    "# #     start += abnorm_patient_id[i]\n",
    "# #     output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "# #     #print(output.hidden_states[-1].shape)\n",
    "# #     tmp_avg1 = np.mean(output.hidden_states[-1].detach().numpy(),axis=1)\n",
    "# #     #print(tmp_avg1.shape)\n",
    "# #     tmp_avg2 = np.mean(tmp_avg1,axis = 0)\n",
    "# #     #print(tmp_avg2.shape)\n",
    "# #     avg_output.append(tmp_avg2)\n",
    "# #     labels_small.append('normal')\n",
    "    \n",
    "# print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings/cluster_abnorm_data_v4.pkl', 'wb') as f:\n",
    "    pickle.dump(avg_output, f)\n",
    "with open('embeddings/cluster_abnorm_label_v4.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_small, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings/cluster_data_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(avg_output, f)\n",
    "with open('embeddings/cluster_label_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_small, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# averaging individual segments, then average into (batch_size, hidden_output_size)\n",
    "\n",
    "start = 0\n",
    "avg_output = []\n",
    "for i in range(len(patient_id)):\n",
    "    end = start + patient_id[i]\n",
    "    #print(start,end)\n",
    "    input_ids = test_dataset['train']['input_ids'][start:end]  \n",
    "    attention_mask = test_dataset['train']['attention_mask'][start:end]\n",
    "    start += patient_id[i]\n",
    "    output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    #print(output.hidden_states[-1].shape)\n",
    "    tmp_avg1 = np.mean(output.hidden_states[-1].detach().numpy(),axis=1)\n",
    "    #print(tmp_avg1.shape)\n",
    "    tmp_avg2 = np.mean(tmp_avg1,axis = 0)\n",
    "    #print(tmp_avg2.shape)\n",
    "    avg_output.append(tmp_avg2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only averaging individual segments\n",
    "avg_output = []\n",
    "for i in range(len(labels)):\n",
    "    #print(start,end)\n",
    "    input_ids = test_dataset['train']['input_ids'][i].unsqueeze(0)\n",
    "    attention_mask = test_dataset['train']['attention_mask'][i].unsqueeze(0)\n",
    "    output = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    # print(output.hidden_states[-1].shape)\n",
    "    tmp_avg1 = np.mean(output.hidden_states[-1].detach().numpy(),axis=1)\n",
    "    # print(tmp_avg1.shape)\n",
    "    tmp_avg2 = np.mean(tmp_avg1,axis = 0)\n",
    "    # print(tmp_avg2.shape)\n",
    "    \n",
    "    avg_output.append(tmp_avg2)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(428, 76)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(avg_output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings/cluster_data_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(avg_output, f)\n",
    "with open('embeddings/cluster_label_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(TEST_LABELS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings/cluster_data2_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(avg_output, f)\n",
    "with open('embeddings/cluster_label2_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAX from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_signal(raw_data, signals):\n",
    "    fz_idx = raw_data.ch_names.index(DATA_CHANNEL)\n",
    "    cz_idx = raw_data.ch_names.index(REFERENCE_CHANNEL)\n",
    "    res = signals[fz_idx] - signals[cz_idx]\n",
    "    return res\n",
    "all_single_signals = []\n",
    "\n",
    "def process_edf(ef):\n",
    "    raw_data = mne.io.read_raw_edf(ef, preload=True)\n",
    "    raw_data = raw_data.resample(SAMPLING_RATE) \n",
    "    signals, times = raw_data[:]\n",
    "    s_signal = single_signal(raw_data, signals)\n",
    "    return s_signal\n",
    "\n",
    "\n",
    "# for ef in tqdm(TRAIN_FILES):\n",
    "#     raw_data = mne.io.read_raw_edf(ef, preload=True)\n",
    "#     raw_data = raw_data.resample(SAMPLING_RATE) \n",
    "#     signals, times = raw_data[:]\n",
    "#     s_signal = single_signal(raw_data, signals)\n",
    "#     all_single_signals.append(s_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "def _yield_args(all_file_paths):\n",
    "    for x in all_file_paths:\n",
    "        yield x\n",
    "        \n",
    "all_single_signals = []\n",
    "with tqdm(total=len(TRAIN_FILES)) as pbar, \\\n",
    "    concurrent.futures.ProcessPoolExecutor(max_workers=24) as executor:\n",
    "    for result in executor.map(process_edf, _yield_args(TRAIN_FILES)):\n",
    "         all_single_signals.append(result)\n",
    "         pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_jagged_array(jagged_array, padding_value=-1):\n",
    "    # Find the maximum length of any row\n",
    "    max_length = max(len(row) for row in jagged_array)\n",
    "\n",
    "    # Initialize the padded array\n",
    "    padded_array = np.full((len(jagged_array), max_length), padding_value)\n",
    "\n",
    "    # Copy the values from the jagged array into the padded array\n",
    "    for i, row in enumerate(jagged_array):\n",
    "        padded_array[i, :len(row)] = row\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "\n",
    "padded_signals = pad_jagged_array(all_single_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sax_train_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(all_single_signals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sax = SymbolicAggregateApproximation(n_bins=N_BINS, strategy='quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_signals_binned = sax.fit_transform(padded_signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 428/428 [06:52<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for ef in tqdm(TEST_FILES):\n",
    "    raw_data = mne.io.read_raw_edf(ef, preload=True)\n",
    "    raw_data = raw_data.resample(SAMPLING_RATE) \n",
    "    signals, times = raw_data[:]\n",
    "    s_signal = single_signal(raw_data, signals)\n",
    "\n",
    "    # SAX transformation\n",
    "    X_sax = sax.transform([s_signal])\n",
    "\n",
    "    \n",
    "    payload = {\n",
    "        #    \"raw_signals\": signals,\n",
    "        #    \"times\": times,\n",
    "        #    \"quantized_signals\": X_sax,\n",
    "        }\n",
    "    payload = X_sax\n",
    "    with open(f\"/home/john_zhang/EEG_NLP/llm_for_eeg/eval/{os.path.basename(ef)}.pkl\", \"wb\") as df:\n",
    "        pickle.dump(payload, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ef in tqdm(TRAIN_FILES):\n",
    "    raw_data = mne.io.read_raw_edf(ef, preload=True)\n",
    "    raw_data = raw_data.resample(SAMPLING_RATE) \n",
    "    signals, times = raw_data[:]\n",
    "    s_signal = single_signal(raw_data, signals)\n",
    "\n",
    "    # SAX transformation\n",
    "    X_sax = sax.transform([s_signal])\n",
    "\n",
    "    \n",
    "    payload = {\n",
    "        #    \"raw_signals\": signals,\n",
    "        #    \"times\": times,\n",
    "        #    \"quantized_signals\": X_sax,\n",
    "        }\n",
    "    payload = X_sax\n",
    "    with open(f\"/home/john_zhang/EEG_NLP/llm_for_eeg/train/{os.path.basename(ef)}.pkl\", \"wb\") as df:\n",
    "        pickle.dump(payload, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eded5328b539aa62c7d54b8449ea7e31e09bb634b1baaf115ac6e33bac29d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
